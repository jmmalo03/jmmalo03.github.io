{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Supervised Machine Learning Fundamentals (v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *YOUR FULL NAME HERE*\n",
    "Netid: Your netid here\n",
    "\n",
    "*Names of students you worked with on this assignment*: LIST HERE IF APPLICABLE (delete if not)\n",
    "\n",
    "Note: this assignment falls under collaboration Mode 2: Individual Assignment â€“ Collaboration Permitted. Please refer to the syllabus for additional information.\n",
    "\n",
    "Instructions for all assignments can be found [here](https://github.com/kylebradbury/ids705/blob/main/notebooks/assignment_instructions.ipynb), and is also linked to from the course syllabus.\n",
    "\n",
    "Total points in the assignment add up to 90; an additional 10 points are allocated to presentation quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build on these concepts throughout the course, so use this assignment as a catalyst to deepen your knowledge and seek help with anything unfamiliar.\n",
    "\n",
    "If some references would be helpful on these topics, I would recommend the following resources:\n",
    "- [Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf) by Deisenroth, Faisal, and Ong\n",
    "- [Deep Learning](https://www.deeplearningbook.org/); Part I: Applied Math and Machine Learning Basics by Goodfellow, Bengio, and Courville\n",
    "- [The Matrix Calculus You Need For Deep Learning](https://arxiv.org/pdf/1802.01528.pdf) by Parr and Howard\n",
    "- [Dive Into Deep Learning](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/index.html); Appendix: Mathematics for Deep Learning by Weness, Hu, et al.\n",
    "\n",
    "*Note: don't worry if you don't understand everything in the references above - some of these books dive into significant minutia of each of these topics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAC USERS TAKE NOTE:\n",
    "# For clearer plots in Jupyter notebooks on macs, run the following line of code:\n",
    "# %config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Supervised vs Unsupervised Learning\n",
    "**[20 points]** For each of the following, (i) explain if each scenario is a classification or regression problem AND why, (ii) indicate whether we are most interested in inference or prediction for that problem AND why, and (iii) provide the sample size $n$ and number of predictors $p$ indicated for each scenario.\n",
    "\n",
    "**(a)** We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n",
    "\n",
    "**(b)** We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n",
    "\n",
    "**(c)** We are interesting in predicting the % change in the US dollar in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the dollar, the % change in the US market, the % change in the British market, and the % change in the German market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Simple Linear Least Squares Regression\n",
    "**[40 points] Build your own llinear regression algorithm**.\n",
    "\n",
    "In this problem you will solve a single-variable linear least squares (LLS) regression problem. Assume that our data was generated by the following model: $Y = wx + \\Epsilon $  where $x \\in \\Reals$, $w \\in Reals$, and  $\\Epsilon \\sim N(0,\\sigma^{2})$. Here we assume $\\sigma$ is some unknown variance of our noise. \n",
    "\n",
    "**(a)** Given some collection of observations from this model of the form $D=(x_{i},y_{i})_{i=1}^{N}, write out the derivation for the linear least squares solution for $w$, given in terms of $x_{i}$ and $y_{i}$, using calculus.  Show your work!  This should involve at least a few lines of math.  Please clearly label the final formula for calculating your weights.     \n",
    "\n",
    "**(b)** Build a working version of a *single variable* LLS regression model using the skeleton code below. We'll use the `sklearn` convention that a supervised learning algorithm has the methods `fit` which trains your algorithm (for LLS that means estimating the weight parameters) and `predict` which uses the estimated weights to predict y for new settings of x.  For training, use the formula you derived in part (a) to infer the weights. \n",
    "\n",
    "**(c)** Create a synthetic dataset using the following model: $Y = 2x + \\Epsilon $  where $x \\in \\Reals$, and  $\\Epsilon \\sim N(0,1)$. Following common convention, we use capital letters to denote random variables, and lower-case letters to denote non-random variables.  Using this approach to generate has several benefits: it allows us to  can control the amount of data available for training and testing our LLS model, and we know in advance precisely what the best possible solution to our problem is!   \n",
    "\n",
    "Begin by sampling $N=50$ values for $x_{i} \\sim U[-5,5]$, where $U[a,b]$ is a uniform distribution with lower bound of $a$ and an upper bound of $b$.  Then use these $x$ values to generate a dataset of $D = (x_{i},y_{i})_{i=1}^{N}$,where for each $x_{i}$ you get the corresponding $y_{i}$ value by passing it through the model given above.  To check the validity of your data, make a scatter plot of the points, and on these same axes, plot the line $y= 2x$.  Please briefly comment (2-4 sentences) upon whether the data looks correct, and why you think it is correct.       \n",
    "\n",
    "**(d)** Split the data above into two *disjoint* subsets, each with half of the data (25 data points each).  Lets call these $D_{TR}$ and $D_{TE}$.  Feed D_{TR}$ into your single_variable_lls \"fit\" method. Then use your \"predict\" method to make predictions for the points in both the $D_{TE}$ and $D_{TR}$ datasets.  Please report the mean square error (MSE) on both the training and testing datasets, respectively.  For full credit, please clearly label these values in your answer below. \n",
    "\n",
    "**(e)** Please make a figure with two axes on it: (i) in the first axes please scatter plot the training data, the true underlying function (i.e., $y = 2x$), and the line created by using your LLS weights; (ii) on the second axes scatter plot the testing data, the true underlying function, and the line created by your LLS model.  For full credit, please make sure you include titles, legend, and labels for the axes on all of your plots. Please use these plots to determine that your implementation of LLS is working properly. \n",
    "\n",
    "**(f)** Please comment on the results you obtained in parts (d) and (e).  How do the MSE values obtained on the training and testing sets compare to one another?  If you ran this experiment again would you expect the same pattern, and why?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton code for part (a) to write your own linear least squares classifier\n",
    "\n",
    "class single_variable_lls:\n",
    "# k-Nearest Neighbor class object for classification training and testing\n",
    "    def __init__(self):\n",
    "        self.w = 0;\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # estimate the weight vector w\n",
    "        \n",
    "\n",
    "        # Store the estimated weight\n",
    "        self.w = w;\n",
    "        \n",
    "    def predict(self, x):\n",
    "        y_hat = [] # Variable to store the estimated class label for \n",
    "        # Calculate the distance from each vector in x to the training data\n",
    "        \n",
    "        # Return the estimated targets\n",
    "        return y_hat\n",
    "\n",
    "# Metric of overall classification accuracy\n",
    "def error(y,y_hat):\n",
    "    nvalues = len(y)\n",
    "    mse = #  / nvalues     # fill this in\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3  Multiple Linear Least Squares Regression\n",
    "**[30 points] Build your own llinear regression algorithm**.\n",
    "\n",
    "In this problem you will solve a multi-variable LLS regression problem. Assume that our data was generated by the following model: $Y = w^{T}x + \\Epsilon $  where $x \\in \\Reals^{P,1}$, $w \\in Reals^{P,1}$, and  $\\Epsilon \\sim N(0,\\sigma^{2})$. Here we assume $\\sigma$ is some unknown variance of our noise. \n",
    "\n",
    "**(a)** Build a working version of a *single variable* LLS regression model using the skeleton code below. We'll use the `sklearn` convention that a supervised learning algorithm has the methods `fit` which trains your algorithm (for LLS that means estimating the weight parameters) and `predict` which uses the estimated weights to predict y for new settings of x.  For training, use the formula you derived in part (a) to infer the weights. \n",
    "\n",
    "**(b)** Create a synthetic dataset using the following model: $Y = w^{T}x + \\Epsilon $  where $x \\in \\Reals^{2 \\times 1}$, $w = [3, 2, 1]^{T}$, and  $\\Epsilon \\sim N(0,1)$.  Begin by sampling $N=50$ values for $x_{i} \\sim U[-5,5]$, where $U[a,b]$ is a uniform distribution with lower bound of $a$ and an upper bound of $b$.  Then use these $x$ values to generate a dataset of $D = (x_{i},y_{i})_{i=1}^{N}$,where for each $x_{i}$ you get the corresponding $y_{i}$ value by passing it through the model given above.  To check the validity of your data, make a scatter plot of the points, and on these same axes, plot the line $y= 2x$.  Please briefly comment (2-4 sentences) upon whether the data looks correct, and why you think it is correct.       \n",
    "\n",
    "**(d)** Split the data above into two *disjoint* subsets, each with half of the data (25 data points each).  Lets call these $D_{TR}$ and $D_{TE}$.  Feed D_{TR}$ into your single_variable_lls \"fit\" method. Then use your \"predict\" method to make predictions for the points in both the $D_{TE}$ and $D_{TR}$ datasets.  Please report the mean square error (MSE) on both the training and testing datasets, respectively.  For full credit, please clearly label these values in your answer below. \n",
    "\n",
    "**(e)** Please make a figure with two axes on it: (i) in the first axes please scatter plot the training data, the true underlying function (i.e., $y = 2x$), and the line created by using your LLS weights; (ii) on the second axes scatter plot the testing data, the true underlying function, and the line created by your LLS model.  For full credit, please make sure you include titles, legend, and labels for the axes on all of your plots. Please use these plots to determine that your implementation of LLS is working properly. \n",
    "\n",
    "**(f)** Please comment on the results you obtained in parts (d) and (e).  How do the MSE values obtained on the training and testing sets compare to one another?  If you ran this experiment again would you expect the same pattern, and why?\n",
    "\n",
    "\n",
    "%%%%%%%%%%%%%%%%\n",
    "\n",
    "\n",
    "**(a)** Build a working version of a linear least squares (LLS) regression model using the skeleton code below. We'll use the `sklearn` convention that a supervised learning algorithm has the methods `fit` which trains your algorithm (for LLS that means estimating the weight parameters) and `predict` which uses the estimated weights to predict y for new settings of x.  For training, please use the analytic solution for the weights of the least squares model.   \n",
    "\n",
    "**(b)** Begin by generating a synthetic dataset that is generated using a model of our choosing. Using this approach has several benefits: for example, it allows us to  can control the amount of data available for training and testing our LLS model, and we know in advance precisely what the best possible solution to our problem  is!  For this problem we will use the following model to generate our data: $Y = w^{T}x + \\Epsilon $  where $x \\in \\Reals^{2 \\times 1}$, $w = [2, 1]^{T}$, and  $\\Epsilon \\sim N(0,1)$. Following common convention, we use capital letters to denote random variables, and lower-case letters to denote non-random variables.  \n",
    "\n",
    "Begin by sampling $N=1000$ values for $x_{i} \\sim U[0,10]$, where $U[a,b]$ is a uniform distribution with lower bound of $a$ and an upper bound of $b$.  Then use these $x$ values to generate a dataset of $D_{Tr} = (x_{i},y_{i})_{i=1}^N $,where for each $x_{i}$ you get the corresponding $y_{i}$ value by passing it through the model given above.  To check the validity of your data, make a 3-D scatter plot of a random subset of 50 of the points, and on these same axes, plot the line $y= w^{T}x$     \n",
    "\n",
    "**(c)** Split the total available data from part (b) into two disjoint subsets of equal size (500 each): a training dataset and a testing dataset.  Use 25 samples from your training dataset to train your regression model.  Lets denote the estimated weights as $\\hat{w}$, then please report the estimated weights, and plot a surface $\\hat{y}=\\hat{w}^{T}x$\n",
    "\n",
    "**(d)** Compare your implementation's MSE and computation time to the scikit learn [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class. How do the results and speed compare to your implementation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skeleton code for part (a) to write your own linear least squares classifier\n",
    "\n",
    "class lls:\n",
    "# k-Nearest Neighbor class object for classification training and testing\n",
    "    def __init__(self):\n",
    "        self.w = 0;\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        # estimate the weight vector w\n",
    "        \n",
    "\n",
    "        # Store the estimated weight\n",
    "        self.w = w;\n",
    "        \n",
    "    def predict(self, x):\n",
    "        y_hat = [] # Variable to store the estimated class label for \n",
    "        # Calculate the distance from each vector in x to the training data\n",
    "        \n",
    "        # Return the estimated targets\n",
    "        return y_hat\n",
    "\n",
    "# Metric of overall classification accuracy\n",
    "def error(y,y_hat):\n",
    "    nvalues = len(y)\n",
    "    mse = #  / nvalues     # fill this in\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
